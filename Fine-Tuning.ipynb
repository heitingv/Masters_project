{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of UNet_WholeMass_finetune_tod4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOVGivNkze82bAAt6rW/qIA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heitingv/Masters_project/blob/master/Fine-Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM0oDLhLwjJ5",
        "colab_type": "text"
      },
      "source": [
        "Code for Fine-Tuning: Modify accordingly witht the necessary pre-trained network and freeze correct layers of current network used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjxxogRwJeU5",
        "colab_type": "text"
      },
      "source": [
        "Install pydicom and barbar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YLvrK0nvd0p",
        "colab_type": "code",
        "outputId": "9a3bd12f-ff6c-4ce2-f383-e68e8754d00f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install pydicom\n",
        "!pip install barbar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/56/342e1f8ce5afe63bf65c23d0b2c1cd5a05600caad1c211c39725d3a4cc56/pydicom-2.0.0-py3-none-any.whl (35.4MB)\n",
            "\u001b[K     |████████████████████████████████| 35.5MB 89kB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.0.0\n",
            "Collecting barbar\n",
            "  Downloading https://files.pythonhosted.org/packages/48/1f/9b69ce144f484cfa00feb09fa752139658961de6303ea592487738d0b53c/barbar-0.2.1-py3-none-any.whl\n",
            "Installing collected packages: barbar\n",
            "Successfully installed barbar-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOkjJYDaJmEp",
        "colab_type": "text"
      },
      "source": [
        "Link to drive for data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKAtUYnaCdDE",
        "colab_type": "code",
        "outputId": "fbbf4c57-aefc-47c2-9364-bd22ca5fbdb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSBBJK2zJpRK",
        "colab_type": "text"
      },
      "source": [
        "Import all libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nQCeu2D_gXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pydicom\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from random import randint\n",
        "\n",
        "import time\n",
        "from barbar import Bar \n",
        "import progressbar\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset as BaseDataset\n",
        "from tqdm import trange\n",
        "from time import sleep\n",
        "from torch.utils.data.sampler import SubsetRandomSampler \n",
        "use_gpu = torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtd_3-UNKFZ9",
        "colab_type": "text"
      },
      "source": [
        "Class for data only with mass:\n",
        "returns image/mask padded (for patch extraction) and image/mask resized to 250x250"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CG4LUgj-dIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(BaseDataset):\n",
        "\n",
        "    CLASSES = ['non tumor','tumor']\n",
        "\n",
        "    def __init__(self, images_dir, masks_dir, classes=None):\n",
        "        self.ids_f=[]\n",
        "        self.ids_m_f=[]\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.ids_m = os.listdir(masks_dir)\n",
        "        for i in range(len(self.ids)):\n",
        "          self.ids[i]=self.ids[i].rstrip(\".dcm\")\n",
        "          for i in range(len(self.ids_m)):\n",
        "            self.ids_m[i]=self.ids_m[i].rstrip(\".png\")\n",
        "\n",
        "        for temp in self.ids_m:\n",
        "          if temp in self.ids:\n",
        "            self.ids_f.append(temp+'.dcm')\n",
        "            self.ids_m_f.append(temp+'.png')\n",
        "\n",
        "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids_f]\n",
        "        self.masks_fps = [os.path.join(masks_dir, mask_id) for mask_id in self.ids_m_f]\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.ids_f)\n",
        "    \n",
        "    def breast_left_or_right(self, image_array):\n",
        "      position=None\n",
        "      image=(image_array>0).float() #transform image into binary for easier analysis\n",
        "      coordinates_breast_tissue=(image==1).nonzero() #look at coordinatex where there is '1'\n",
        "      min_coordinates=torch.min(coordinates_breast_tissue,0)[0][1].item() #find the minimum column of where breast, if breast on the left then 0/1, if right then high number\n",
        "      \n",
        "      if  min_coordinates<=100:\n",
        "        position='left'\n",
        "      else:\n",
        "        position='right'\n",
        "      \n",
        "      return(position)\n",
        "\n",
        "    def image_padding(self, position, image_array, mask_array):\n",
        "\n",
        "      if image_array.shape[0]==4084:\n",
        "        if position=='left':\n",
        "          image_tensor=torch.nn.functional.pad(image_array, (0,(3500-3328),0,(4250-4084)))\n",
        "          mask_tensor=torch.nn.functional.pad(mask_array, (0,(3500-3328),0,(4250-4084)))\n",
        "        else:\n",
        "          image_tensor=torch.nn.functional.pad(image_array, ((3500-3328),0,0,(4250-4084)))\n",
        "          mask_tensor=torch.nn.functional.pad(mask_array, ((3500-3328),0,0,(4250-4084)))\n",
        "      \n",
        "      else:\n",
        "        if position=='left':\n",
        "          image_tensor=torch.nn.functional.pad(image_array, (0,(2750-2560),0,(3500-3328)))\n",
        "          mask_tensor=torch.nn.functional.pad(mask_array, (0,(2750-2560),0,(3500-3328)))\n",
        "        else:\n",
        "          image_tensor=torch.nn.functional.pad(image_array, ((2750-2560),0,0,(3500-3328)))\n",
        "          mask_tensor=torch.nn.functional.pad(mask_array, ((2750-2560),0,0,(3500-3328)))\n",
        "      \n",
        "      return(image_tensor,mask_tensor)\n",
        "\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = pydicom.dcmread(self.images_fps[i])\n",
        "        image = image.pixel_array.astype('float')\n",
        "        image_re = cv2.resize(image,(250,250))\n",
        "        image = torch.from_numpy(image)\n",
        "      \n",
        "        mask = cv2.imread(self.masks_fps[i])\n",
        "        mask_re = cv2.resize(mask,(250,250))\n",
        "        mask_re = torch.from_numpy(mask_re)\n",
        "        mask_re = mask_re.long()\n",
        "        mask_re = abs((mask_re.sum(2)/3)-1)\n",
        "        mask_re = (mask_re>0).float()\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.long()\n",
        "        mask = abs((mask.sum(2)/3)-1)\n",
        "        mask = (mask>0).float()\n",
        "\n",
        "        position=self.breast_left_or_right(image)\n",
        "        image_pad, mask_pad = self.image_padding(position,image,mask)\n",
        "        \n",
        "        return image_pad, mask_pad, image_re, mask_re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I61PCt45KYwL",
        "colab_type": "text"
      },
      "source": [
        "Class for data without mass: returns image/mask padded (for patch extraction) and image/mask resized to 250x250\n",
        "\n",
        "The distinction between the two classes is be able to create a balanced dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj8aXPOZ_jxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset_with_NonMass(BaseDataset): #original data non padded or changed in size \n",
        "\n",
        "    CLASSES = ['non tumor','tumor']\n",
        "\n",
        "    def __init__(self, images_dir, masks_dir, classes=None):\n",
        "        self.ids_f=[]\n",
        "        self.ids_m_f=[]\n",
        "        self.ids = os.listdir(images_dir)\n",
        "        self.ids_m = os.listdir(masks_dir)\n",
        "        self.images_fps=[]\n",
        "        self.masks_fps=[]\n",
        "        for i in range(len(self.ids)):\n",
        "          self.ids[i]=self.ids[i].rstrip(\".dcm\")\n",
        "          for i in range(len(self.ids_m)):\n",
        "            self.ids_m[i]=self.ids_m[i].rstrip(\".png\")\n",
        "\n",
        "        for i in range(len(self.ids)):\n",
        "          temp=self.ids[i][0:8]\n",
        "          if temp in self.ids_m:\n",
        "            self.ids_f.append(self.ids[i]+'.dcm')\n",
        "            self.ids_m_f.append(temp+'.png')\n",
        "\n",
        "        for i in range(len(self.ids_m_f)):\n",
        "          mask_id=self.ids_m_f[i]\n",
        "          image_id=self.ids_f[i]\n",
        "          temp=os.path.join(masks_dir, mask_id)\n",
        "          presence=self.test_mass(temp)\n",
        "\n",
        "          if presence==False:\n",
        "            self.images_fps.append(os.path.join(images_dir, image_id))\n",
        "            self.masks_fps.append(os.path.join(masks_dir, mask_id))\n",
        "\n",
        "          temp=None\n",
        "          mask_id=None\n",
        "          image_id=None\n",
        "\n",
        "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
        "\n",
        "    def test_mass(self,name):\n",
        "      presence=None\n",
        "\n",
        "      mask = cv2.imread(name)\n",
        "      mask = torch.from_numpy(mask)\n",
        "      mask=mask.long()\n",
        "      mask=abs((mask.sum(2)/3)-1)\n",
        "      mask = (mask>0).float()\n",
        "\n",
        "      if 1 in mask:\n",
        "        presence=True\n",
        "      else:\n",
        "        presence=False\n",
        "\n",
        "      return(presence)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_fps)\n",
        "\n",
        "    def breast_left_or_right(self, image_array):\n",
        "      position=None\n",
        "      image=(image_array>0).float() #transform image into binary for easier analysis\n",
        "      coordinates_breast_tissue=(image==1).nonzero() #look at coordinatex where there is '1'\n",
        "      min_coordinates=torch.min(coordinates_breast_tissue,0)[0][1].item() #find the minimum column of where breast, if breast on the left then 0/1, if right then high number\n",
        "      \n",
        "      if  min_coordinates<=100:\n",
        "        position='left'\n",
        "      else:\n",
        "        position='right'\n",
        "      \n",
        "      return(position)\n",
        "\n",
        "    def image_padding(self, position, image_array, mask_array):\n",
        "\n",
        "      if image_array.shape[0]==4084:\n",
        "        if position=='left':\n",
        "          image_tensor=torch.nn.functional.pad(image_array, (0,(3500-3328),0,(4250-4084)))\n",
        "          mask_tensor=torch.nn.functional.pad(mask_array, (0,(3500-3328),0,(4250-4084)))\n",
        "        else:\n",
        "          image_tensor=torch.nn.functional.pad(image_array, ((3500-3328),0,0,(4250-4084)))\n",
        "          mask_tensor=torch.nn.functional.pad(mask_array, ((3500-3328),0,0,(4250-4084)))\n",
        "      \n",
        "      else:\n",
        "        if position=='left':\n",
        "          image_tensor=torch.nn.functional.pad(image_array, (0,(2750-2560),0,(3500-3328)))\n",
        "          mask_tensor=torch.nn.functional.pad(mask_array, (0,(2750-2560),0,(3500-3328)))\n",
        "        else:\n",
        "          image_tensor=torch.nn.functional.pad(image_array, ((2750-2560),0,0,(3500-3328)))\n",
        "          mask_tensor=torch.nn.functional.pad(mask_array, ((2750-2560),0,0,(3500-3328)))\n",
        "      \n",
        "      return(image_tensor,mask_tensor)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        \n",
        "        # read data\n",
        "        image = pydicom.dcmread(self.images_fps[i])\n",
        "        image = image.pixel_array.astype('float')\n",
        "        image_re = cv2.resize(image,(250,250))\n",
        "        image = torch.from_numpy(image)\n",
        "      \n",
        "        mask = cv2.imread(self.masks_fps[i])\n",
        "        mask_re = cv2.resize(mask,(250,250))\n",
        "        mask_re = torch.from_numpy(mask_re)\n",
        "        mask_re = mask_re.long()\n",
        "        mask_re = abs((mask_re.sum(2)/3)-1)\n",
        "        mask_re = (mask_re>0).float()\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.long()\n",
        "        mask = abs((mask.sum(2)/3)-1)\n",
        "        mask = (mask>0).float()\n",
        "\n",
        "        position=self.breast_left_or_right(image)\n",
        "        image_pad, mask_pad = self.image_padding(position,image,mask)\n",
        "\n",
        "        return image_pad, mask_pad, image_re, mask_re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIbydE0wKywB",
        "colab_type": "text"
      },
      "source": [
        "Load train, validation and test dataset from drive that was previously saved so that all trainings/testings are done on exactly the same images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owmmKS2OjMG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader=(torch.load('/gdrive/My Drive/test_loader.pth'))\n",
        "train_loader=(torch.load('/gdrive/My Drive/train_loader.pth'))\n",
        "val_loader=(torch.load('/gdrive/My Drive/val_loader.pth'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKdXuQqNw0Zs",
        "colab_type": "text"
      },
      "source": [
        "Interchange between UNet, UNet++ and FC-DenseNet depending\n",
        "Here the standard UNet is used as example with its contracting path frozen\n",
        "Further down in the code, I will add in the same thing for UNet++ and FC-DenseNet for additional information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7FWqKRu-swU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.25)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
        "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "      \n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 512)\n",
        "        self.up1 = Up(1024, 256, bilinear)\n",
        "        self.up2 = Up(512, 128, bilinear)\n",
        "        self.up3 = Up(256, 64, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP20S-MUYeoq",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwZyKElSREYP",
        "colab_type": "code",
        "outputId": "84e63ab2-5cda-44b3-c891-be6fee7e3592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model = UNet(n_classes=2, n_channels=1)\n",
        "for name, child in model.named_children():\n",
        "    print(name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inc\n",
            "down1\n",
            "down2\n",
            "down3\n",
            "down4\n",
            "up1\n",
            "up2\n",
            "up3\n",
            "up4\n",
            "outc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-ByBJIqRGv5",
        "colab_type": "code",
        "outputId": "5d6e5548-2e42-4f5b-adef-93fae5602f1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for name, child in model.named_children():\n",
        "   if name in ['inc','down1','down2','down3']:\n",
        "       print(name + ' is unfrozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = True\n",
        "   else:\n",
        "       print(name + ' is frozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = False"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inc is unfrozen\n",
            "down1 is unfrozen\n",
            "down2 is unfrozen\n",
            "down3 is unfrozen\n",
            "down4 is frozen\n",
            "up1 is frozen\n",
            "up2 is frozen\n",
            "up3 is frozen\n",
            "up4 is frozen\n",
            "outc is frozen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrnpinNcxl8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "a10a26c1-2c48-4996-8777-73df87985917"
      },
      "source": [
        "!pip install pytorch-model-summary\n",
        "from pytorch_model_summary import summary\n",
        "print(summary(model, torch.zeros((1, 1, 250, 250)), show_input=False))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-model-summary in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-model-summary) (0.16.0)\n",
            "--------------------------------------------------------------------------\n",
            "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
            "==========================================================================\n",
            "      DoubleConv-1      [1, 64, 250, 250]          37,824          37,824\n",
            "            Down-2     [1, 128, 125, 125]         221,952         221,952\n",
            "            Down-3       [1, 256, 62, 62]         886,272         886,272\n",
            "            Down-4       [1, 512, 31, 31]       3,542,016       3,542,016\n",
            "            Down-5       [1, 512, 15, 15]       4,721,664               0\n",
            "              Up-6       [1, 256, 31, 31]       2,950,656               0\n",
            "              Up-7       [1, 128, 62, 62]         738,048               0\n",
            "              Up-8      [1, 64, 125, 125]         184,704               0\n",
            "              Up-9      [1, 64, 250, 250]         110,976               0\n",
            "        OutConv-10       [1, 2, 250, 250]             130               0\n",
            "==========================================================================\n",
            "Total params: 13,394,242\n",
            "Trainable params: 4,688,064\n",
            "Non-trainable params: 8,706,178\n",
            "--------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoylwUfRKsMY",
        "colab_type": "text"
      },
      "source": [
        "Function to calculate IoU, Dice coeff etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTlK8b0BKp4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def metrics(predicted,truth,word):\n",
        "  ####### metrics for tumour\n",
        "  TP=0\n",
        "  FN=0\n",
        "  FP=0\n",
        "  TN=0\n",
        "  for i in range(truth.squeeze().size()[0]):\n",
        "    for j in range(truth.squeeze().size()[1]):\n",
        "      if truth[i,j]==1 and predicted[i,j]==1:\n",
        "        TP+=1\n",
        "      elif truth[i,j]==1 and predicted[i,j]==0:\n",
        "        FN+=1\n",
        "      elif truth[i,j]==0 and predicted[i,j]==1:\n",
        "        FP+=1\n",
        "      else:\n",
        "        TN+=1\n",
        "  \n",
        "  if TP==0 and FP==0 and FN==0:\n",
        "    iou_tumour=0\n",
        "    dice_tumour=0\n",
        "  else:\n",
        "    iou_tumour = TP/(TP+FP+FN)\n",
        "    dice_tumour = (2*TP)/(TP+FP+TP+FN)\n",
        "\n",
        "  if TN==0 and FP==0:\n",
        "    spec_tumour=0\n",
        "  else:\n",
        "    spec_tumour = TN/(TN+FP) #specificity #true negative rate\n",
        " \n",
        "  if TP==0 and FN==0:\n",
        "    sens_tumour=0\n",
        "  else:\n",
        "    sens_tumour = TP/(TP+FN) #sensitivity #true positive rate\n",
        "\n",
        "  acc_tumour = (TP+TN)/(TP+TN+FP+FN) #accuracy\n",
        "\n",
        "  ####### metrics for background\n",
        "  truth_b=abs(truth-1)\n",
        "  predicted_b=abs(predicted-1)\n",
        "  TP_b=0\n",
        "  FN_b=0\n",
        "  FP_b=0\n",
        "  TN_b=0\n",
        "  for i in range(truth_b.squeeze().size()[0]):\n",
        "    for j in range(truth_b.squeeze().size()[1]):\n",
        "      if truth_b[i,j]==1 and predicted_b[i,j]==1:\n",
        "        TP_b+=1\n",
        "      elif truth_b[i,j]==1 and predicted_b[i,j]==0:\n",
        "        FN_b+=1\n",
        "      elif truth_b[i,j]==0 and predicted_b[i,j]==1:\n",
        "        FP_b+=1\n",
        "      else:\n",
        "        TN_b+=1\n",
        "  \n",
        "  if TP_b==0 and FP_b==0 and FN_b==0:\n",
        "    iou_background=0\n",
        "    dice_background=0\n",
        "  else:\n",
        "    iou_background = TP_b/(TP_b+FP_b+FN_b)\n",
        "    dice_background = (2*TP_b)/(TP_b+FP_b+TP_b+FN_b)\n",
        "\n",
        "  if TN_b==0 and FP_b==0:\n",
        "    spec_background=0\n",
        "  else:\n",
        "   spec_background = TN_b/(TN_b+FP_b) #specificity #true negative rate\n",
        " \n",
        "  if TP_b==0 and FN_b==0:\n",
        "    sens_background=0\n",
        "  else:\n",
        "    sens_background = TP_b/(TP_b+FN_b) #sensitivity #true positive rate\n",
        "\n",
        "  acc_background = (TP_b+TN_b)/(TP_b+TN_b+FP_b+FN_b) #accuracy\n",
        "\n",
        "  ####### metrics for mean of tumour & background\n",
        "  object_nb=0\n",
        "  if 1 not in truth:\n",
        "    object_nb=1\n",
        "  else:\n",
        "    object_nb=2\n",
        "  \n",
        "  mean_iou=(iou_tumour+iou_background)/object_nb\n",
        "  mean_dice=(dice_tumour+dice_background)/object_nb\n",
        "  mean_spec=(spec_tumour+spec_background)/object_nb\n",
        "  mean_sens=(sens_tumour+sens_background)/object_nb\n",
        "  mean_acc=(acc_tumour+acc_background)/2\n",
        "  \n",
        "  if word=='iou':\n",
        "    return(iou_tumour,iou_background,mean_iou)\n",
        "  elif word=='AllTumour':\n",
        "    return(iou_tumour,dice_tumour,spec_tumour,sens_tumour,acc_tumour)\n",
        "  elif word=='AllBackground':\n",
        "    return(iou_background,dice_background,spec_background,sens_background,acc_background)\n",
        "  elif word=='AllMean':\n",
        "    return(mean_iou,mean_dice,mean_spec,mean_sens,mean_acc)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svKht0GzYg_i",
        "colab_type": "text"
      },
      "source": [
        "Optimizer & criterion: the learning rate need to be multiplied by 10^-1 for fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F2zPbjDBe3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYL25sNzyT5C",
        "colab_type": "text"
      },
      "source": [
        "Add the opath to the correct pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNUPfLsI6deR",
        "colab_type": "code",
        "outputId": "db961dc3-696c-4457-f51e-124fbbbe6832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path_transfer='/networks/FINETUNE_UNET/UNet_WholeMass/'\n",
        "name_transfer='UNet_WholeMass'\n",
        "model.load_state_dict(torch.load('/gdrive/My Drive'+path_transfer+name_transfer+'.pth'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjvcoC7VDZ01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = UNet(n_classes=2, n_channels=1)\n",
        "path='/networks/FINETUNE_UNET/UNet_WholeMass_finetune/'\n",
        "name='UNet_WholeMass_finetune'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g0gYNG0Kmus",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYXJ4eItO3f_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=20\n",
        "running_loss = 0.0\n",
        "best=0.0\n",
        "best_tumour=0.0\n",
        "train_iou = 0.0\n",
        "val_iou = 0.0\n",
        "val_tumour_iou=[]\n",
        "counter=0\n",
        "iou_list=[]\n",
        "val_iou_list=[]\n",
        "nb_patches_analyzed=0\n",
        "nb_patches_analyzed_v=0\n",
        "\n",
        "file = open('/gdrive/My Drive'+path+'training2.txt','w')\n",
        "file.write('Training: \\n')\n",
        "\n",
        "time.sleep(5)\n",
        "for epoch in range(epochs):  \n",
        "\n",
        "    ###################### Training\n",
        "    time.sleep(5)\n",
        "    Bar = progressbar.ProgressBar(max_value=len(train_loader))\n",
        "    for i, data in enumerate(Bar(train_loader), 0):\n",
        "        image, mask, image_re, mask_re = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(image_re.unsqueeze(dim=0).float()) \n",
        "        outputs_final_probabilities, outputs_final = torch.max(outputs.squeeze(), axis=0) \n",
        "        loss = criterion(outputs.float(), mask_re.long())\n",
        "        loss.backward() #backward propagation\n",
        "        optimizer.step() #optimize\n",
        "        iou_tumour,iou_background,mean_iou = metrics(outputs_final,mask_re.squeeze(),'iou')\n",
        "        train_iou+=mean_iou\n",
        "\n",
        "    ############### Validation\n",
        "    time.sleep(5)\n",
        "    BarTwo = progressbar.ProgressBar(max_value=len(val_loader))   \n",
        "    for i, data_v in enumerate(BarTwo(val_loader), 0):\n",
        "        image_v, mask_v, image_re_v, mask_re_v = data_v\n",
        "        outputs_v=model(image_re_v.unsqueeze(dim=0).float())\n",
        "        outputs_final_probabilities_v, outputs_final_v = torch.max(outputs_v.squeeze(), axis=0)\n",
        "        iou_tumour,iou_background,mean_iou = metrics(outputs_final_v,mask_re_v.squeeze(),'iou')\n",
        "        val_iou+=mean_iou\n",
        "        if 1 in mask_re_v:\n",
        "          val_tumour_iou.append(iou_tumour)\n",
        "\n",
        "\n",
        "    ################ save best model\n",
        "    if val_iou > best:\n",
        "      best=val_iou\n",
        "      torch.save(model.state_dict(),'/gdrive/My Drive/'+path+name+'.pth')\n",
        "    \n",
        "    if sum(val_tumour_iou) > best_tumour:\n",
        "      best_tumour=sum(val_tumour_iou)\n",
        "      torch.save(model.state_dict(),'/gdrive/My Drive/'+path+name+'_tumour.pth')\n",
        "      \n",
        "    ################ print for each epoch iou\n",
        "    print('Epoch %d :iou train: %.3f' % (epoch + 1, train_iou/ len(train_loader)),'; iou val: %.3f' % (val_iou/ len(val_loader)),'; tumour iou val: %.3f' % (sum(val_tumour_iou)/ len(val_tumour_iou)),'\\n')\n",
        "    file.write('\\n\\nEpoch %d : mean iou train: %.3f' % (epoch + 1, train_iou/ len(train_loader)))\n",
        "    file.write(' ;mean iou validation: %.3f' % (val_iou/ len(val_loader)))  \n",
        "    file.write(' ;tumour iou validation: %.3f' % (sum(val_tumour_iou)/ len(val_tumour_iou)))    \n",
        "\n",
        "    iou_list.append(train_iou/ len(train_loader))\n",
        "    val_iou_list.append(val_iou/ len(val_loader))\n",
        "\n",
        "    #print for every 20 epoch\n",
        "    if (counter%1)==0:\n",
        "      fig = plt.figure()\n",
        "      plt.subplot(1, 3, 1)\n",
        "      plt.imshow(image_re.squeeze())\n",
        "      plt.subplot(1, 3, 2)\n",
        "      plt.imshow(mask_re.squeeze())\n",
        "      plt.subplot(1, 3, 3)\n",
        "      plt.imshow(outputs_final.detach().numpy())\n",
        "\n",
        "    counter+=1\n",
        "    running_loss = 0.0\n",
        "    train_iou = 0.0\n",
        "    val_tumour_iou = []\n",
        "    nb_patches_analyzed=0\n",
        "    nb_patches_analyzed_v=0\n",
        "    val_iou=0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# file.write('\\n\\nTrain iou list')\n",
        "# file.wirte(iou_list)\n",
        "# file.write('Validation iou_list')\n",
        "# file.write(val_iou_list)\n",
        "file.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R_d9DSYvfuc",
        "colab_type": "text"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmeGHUoENQZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_state_dict(torch.load('/gdrive/My Drive/'+path+name+'.pth'))\n",
        "\n",
        "file = open('/gdrive/My Drive'+path+'testing.txt','w')\n",
        "file.write('Testing \\n')\n",
        "\n",
        "iou_tumour_list=[] \n",
        "ground=[] \n",
        "test_iou = 0.0\n",
        "test_dice = 0.0\n",
        "test_spec = 0.0\n",
        "test_sens = 0.0\n",
        "test_acc = 0.0\n",
        "test_iou_tumour = 0.0\n",
        "test_dice_tumour = 0.0\n",
        "test_spec_tumour = 0.0\n",
        "test_sens_tumour = 0.0\n",
        "test_acc_tumour = 0.0\n",
        "\n",
        "false_positive=0\n",
        "true_positive=0\n",
        "false_negative=0\n",
        "true_negative=0\n",
        "FPR=0\n",
        "TPR=0\n",
        "FNR=0\n",
        "TNR=0\n",
        "tum_count=0\n",
        "\n",
        "################ test model\n",
        "for i, data_t in enumerate(test_loader, 0):\n",
        "  image_t, mask_t, image_re_t, mask_re_t = data_t\n",
        "  outputs_t=model(image_re_t.unsqueeze(dim=0).float())\n",
        "  outputs_final_probabilities_t, outputs_final_t = torch.max(outputs_t.squeeze(), axis=0)\n",
        "  iou_tumour,iou_background,mean_iou = metrics(outputs_final_t,mask_re_t.squeeze(),'iou')\n",
        "  test_iou += mean_iou \n",
        "  mean_iou,mean_dice,mean_spec,mean_sens,mean_acc= metrics(outputs_final_t,mask_re_t.squeeze(),'AllMean')\n",
        "  test_dice += mean_dice\n",
        "  test_spec += mean_spec\n",
        "  test_sens += mean_sens\n",
        "  test_acc += mean_acc\n",
        "  iou_tumour_list.append(iou_tumour)\n",
        "  print('\\n\\nTest Image %d : Mean iou %.3f' % (i+1, mean_iou))\n",
        "  file.write('\\n\\nTest image %d : Mean iou %.3f, mean dice %.3f, mean spec %.3f, mean sens %.3f, mean acc %.3f' % (i+1,mean_iou,mean_dice,mean_spec,mean_sens,mean_acc))\n",
        "\n",
        "  if 1 in mask_re_t:\n",
        "    print('Tumour Iou: %.3f' % (iou_tumour))\n",
        "    iou_tumour,dice_tumour,spec_tumour,sens_tumour,acc_tumour = metrics(outputs_final_t,mask_re_t.squeeze(),'AllTumour')\n",
        "    test_iou_tumour += iou_tumour\n",
        "    test_dice_tumour += dice_tumour\n",
        "    test_spec_tumour += spec_tumour\n",
        "    test_sens_tumour += sens_tumour\n",
        "    test_acc_tumour += acc_tumour\n",
        "    tum_count+=1\n",
        "    file.write('\\n tumour iou %.3f, tumour dice %.3f, tumour spec %.3f, tumour sens %.3f, tumour acc %.3f' % (iou_tumour,dice_tumour,spec_tumour,sens_tumour,acc_tumour))\n",
        "\n",
        "  if 1 in mask_re_t:\n",
        "    ground.append(1)\n",
        "  else:\n",
        "    ground.append(0)\n",
        "\n",
        "  if 1 in mask_re_t.squeeze() and 1 in outputs_final_t.squeeze():\n",
        "    true_positive=true_positive+1\n",
        "  elif 1 not in mask_re_t.squeeze() and 1 in outputs_final_t.squeeze():\n",
        "    false_positive=false_positive+1\n",
        "  elif 1 in mask_re_t.squeeze() and 1 not in outputs_final_t.squeeze():\n",
        "    false_negative=false_negative+1\n",
        "  else:\n",
        "    true_negative=true_negative+1\n",
        "\n",
        "  fig = plt.figure()\n",
        "  plt.subplot(1, 3, 1)\n",
        "  plt.imshow(image_re_t.squeeze())\n",
        "  plt.subplot(1, 3, 2)\n",
        "  plt.imshow(mask_re_t.squeeze())\n",
        "  plt.subplot(1, 3, 3)\n",
        "  plt.imshow(outputs_final_t.detach().numpy())\n",
        "  matplotlib.image.imsave('/gdrive/My Drive'+path+'test2/'+name+'_image_'+str(i)+'.png',image_t.squeeze())\n",
        "  matplotlib.image.imsave('/gdrive/My Drive'+path+'test2/'+name+'_mask_'+str(i)+'.png',mask_t.squeeze())\n",
        "  matplotlib.image.imsave('/gdrive/My Drive'+path+'test2/'+name+'_prediction_'+str(i)+'.png',outputs_final_t.detach().numpy())\n",
        "\n",
        "\n",
        "#print('\\nOverall Test Images: test loss: %.3f' % (test_loss / len(test_loader)))\n",
        "print('\\n\\n\\nOver entire Test data set: average mean iou: %.3f' % (test_iou/ len(test_loader)))\n",
        "file.write('\\n\\n\\n\\nOverall iou %.3f, overall dice %.3f, overall spec %.3f, overall sens %.3f, overall acc %.3f' % (test_iou/len(test_loader),test_dice/len(test_loader),test_spec/len(test_loader),test_sens/len(test_loader),test_acc/len(test_loader)))\n",
        "print('\\nAverage tumour iou (for mask that have tumour): %.3f' % (test_iou_tumour/tum_count), '\\n')\n",
        "file.write('\\n\\nOverall tumour iou %.3f, overall tumour dice %.3f, overall tumour spec %.3f, overall tumour sens %.3f, tumour overall acc %.3f' % (test_iou_tumour/tum_count,test_dice_tumour/tum_count,test_spec_tumour/tum_count,test_sens_tumour/tum_count,test_acc_tumour/tum_count))\n",
        "\n",
        "####### from testing\n",
        "FPR=false_positive/(false_positive+true_negative) #False Positive Rate \n",
        "TPR=true_positive/(true_positive+false_negative) #True Positive Rate ###Sens\n",
        "FNR=false_negative/(false_negative+true_positive) #False Negative Rate\n",
        "TNR=true_negative/(true_negative+false_positive) #True Negative Rate ###Spec\n",
        "\n",
        "Acc=(true_negative+true_positive)/(false_positive+false_negative+true_negative+true_positive)\n",
        "\n",
        "auc=ROC(iou_tumour_list,ground)\n",
        "\n",
        "# f1, auprc = Recall_Precision(iou_tumour_list,ground)\n",
        "\n",
        "# file.write('\\n\\n-------------------')\n",
        "# file.write('\\nAUC: ')\n",
        "# file.write(auc)\n",
        "# file.write('\\nF1: ',f1,' ;AUPRC: ',auprc)\n",
        "file.close()\n",
        "\n",
        "# print('False Positive Rate:', FPR, '\\n')\n",
        "# print('True Positive Rate:', TPR, '\\n')\n",
        "# print('False Negative Rate:', FNR, '\\n')\n",
        "# print('True Negative Rate:', TNR, '\\n')\n",
        "# print('Acc:', acc, '\\n')\n",
        "# print('AUC:', auc, '\\n')\n",
        "# print('F1:', f1, 'AUPRC:', auprc, '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfqqAcVlylJW",
        "colab_type": "text"
      },
      "source": [
        "------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWdVyUfqyoRs",
        "colab_type": "text"
      },
      "source": [
        "UNet++ finetuning freezing layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzF8jhRymc3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class conv_block_nested(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch, mid_ch, out_ch):\n",
        "        super(conv_block_nested, self).__init__()\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_ch, mid_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn1 = nn.BatchNorm2d(mid_ch)\n",
        "        self.conv2 = nn.Conv2d(mid_ch, out_ch, kernel_size=3, padding=1, bias=True)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        output = self.activation(x)\n",
        "\n",
        "        return output\n",
        "\n",
        "class Nested_UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, in_ch=1, out_ch=2):\n",
        "        super(Nested_UNet, self).__init__()\n",
        "\n",
        "        n1 = 64\n",
        "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "        self.conv0_0 = conv_block_nested(in_ch, filters[0], filters[0])\n",
        "        self.conv1_0 = conv_block_nested(filters[0], filters[1], filters[1])\n",
        "        self.conv2_0 = conv_block_nested(filters[1], filters[2], filters[2])\n",
        "        self.conv3_0 = conv_block_nested(filters[2], filters[3], filters[3])\n",
        "        self.conv4_0 = conv_block_nested(filters[3], filters[4], filters[4])\n",
        "\n",
        "        self.conv0_1 = conv_block_nested(filters[0] + filters[1], filters[0], filters[0])\n",
        "        self.conv1_1 = conv_block_nested(filters[1] + filters[2], filters[1], filters[1])\n",
        "        self.conv2_1 = conv_block_nested(filters[2] + filters[3], filters[2], filters[2])\n",
        "        self.conv3_1 = conv_block_nested(filters[3] + filters[4], filters[3], filters[3])\n",
        "\n",
        "        self.conv0_2 = conv_block_nested(filters[0]*2 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_2 = conv_block_nested(filters[1]*2 + filters[2], filters[1], filters[1])\n",
        "        self.conv2_2 = conv_block_nested(filters[2]*2 + filters[3], filters[2], filters[2])\n",
        "\n",
        "        self.conv0_3 = conv_block_nested(filters[0]*3 + filters[1], filters[0], filters[0])\n",
        "        self.conv1_3 = conv_block_nested(filters[1]*3 + filters[2], filters[1], filters[1])\n",
        "\n",
        "        self.conv0_4 = conv_block_nested(filters[0]*4 + filters[1], filters[0], filters[0])\n",
        "\n",
        "        self.final = nn.Conv2d(filters[0], out_ch, kernel_size=1)\n",
        "\n",
        "    def resize(self,x1,x2):\n",
        "      x1 = self.Up(x1)\n",
        "      diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
        "      diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
        "\n",
        "      x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                      diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "      return x1\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x0_0 = self.conv0_0(x)\n",
        "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
        "        x0_1 = self.conv0_1(torch.cat([x0_0, self.resize(x1_0,x0_0)], 1))\n",
        "\n",
        "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
        "        x1_1 = self.conv1_1(torch.cat([x1_0, self.resize(x2_0,x1_0)], 1))\n",
        "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.resize(x1_1,x0_1)], 1))\n",
        "\n",
        "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
        "        x2_1 = self.conv2_1(torch.cat([x2_0, self.resize(x3_0,x2_0)], 1))\n",
        "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.resize(x2_1,x1_1)], 1))\n",
        "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.resize(x1_2,x0_2)], 1))\n",
        "\n",
        "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
        "        x3_1 = self.conv3_1(torch.cat([x3_0, self.resize(x4_0,x3_0)], 1))\n",
        "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.resize(x3_1,x2_1)], 1))\n",
        "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.resize(x2_2,x1_2)], 1))\n",
        "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.resize(x1_3,x0_3)], 1))\n",
        "\n",
        "        output = self.final(x0_4)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2A3qmPGwy46f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "5ed37e57-85e9-4708-81e8-51c17f9d7a7a"
      },
      "source": [
        "model=Nested_UNet()\n",
        "for name, child in model.named_children():\n",
        "    print(name)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pool\n",
            "Up\n",
            "conv0_0\n",
            "conv1_0\n",
            "conv2_0\n",
            "conv3_0\n",
            "conv4_0\n",
            "conv0_1\n",
            "conv1_1\n",
            "conv2_1\n",
            "conv3_1\n",
            "conv0_2\n",
            "conv1_2\n",
            "conv2_2\n",
            "conv0_3\n",
            "conv1_3\n",
            "conv0_4\n",
            "final\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OzV_dJsy_ID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "dd4b0d28-0247-4f45-e296-29597bcbef46"
      },
      "source": [
        "for name, child in model.named_children():\n",
        "   if name in ['pool','Up','conv0_0','conv1_0','conv2_0','conv3_0','conv0_1','conv1_1','conv2_1','conv0_2']:\n",
        "       print(name + ' is unfrozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = True\n",
        "   else:\n",
        "       print(name + ' is frozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = False"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pool is unfrozen\n",
            "Up is unfrozen\n",
            "conv0_0 is unfrozen\n",
            "conv1_0 is unfrozen\n",
            "conv2_0 is unfrozen\n",
            "conv3_0 is unfrozen\n",
            "conv4_0 is frozen\n",
            "conv0_1 is unfrozen\n",
            "conv1_1 is unfrozen\n",
            "conv2_1 is unfrozen\n",
            "conv3_1 is frozen\n",
            "conv0_2 is unfrozen\n",
            "conv1_2 is frozen\n",
            "conv2_2 is frozen\n",
            "conv0_3 is frozen\n",
            "conv1_3 is frozen\n",
            "conv0_4 is frozen\n",
            "final is frozen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlIphgeGzCrN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "e52e814d-5915-4b92-9e9f-f9406ac222e1"
      },
      "source": [
        "!pip install pytorch-model-summary\n",
        "from pytorch_model_summary import summary\n",
        "print(summary(model, torch.zeros((1, 1, 28, 28)), show_input=False))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-model-summary in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-model-summary) (0.16.0)\n",
            "-----------------------------------------------------------------------------\n",
            "           Layer (type)         Output Shape         Param #     Tr. Param #\n",
            "=============================================================================\n",
            "    conv_block_nested-1      [1, 64, 28, 28]          37,824          37,824\n",
            "            MaxPool2d-2      [1, 64, 14, 14]               0               0\n",
            "    conv_block_nested-3     [1, 128, 14, 14]         221,952         221,952\n",
            "             Upsample-4     [1, 128, 28, 28]               0               0\n",
            "    conv_block_nested-5      [1, 64, 28, 28]         147,840         147,840\n",
            "            MaxPool2d-6       [1, 128, 7, 7]               0               0\n",
            "    conv_block_nested-7       [1, 256, 7, 7]         886,272         886,272\n",
            "             Upsample-8     [1, 256, 14, 14]               0               0\n",
            "    conv_block_nested-9     [1, 128, 14, 14]         590,592         590,592\n",
            "            Upsample-10     [1, 128, 28, 28]               0               0\n",
            "   conv_block_nested-11      [1, 64, 28, 28]         184,704         184,704\n",
            "           MaxPool2d-12       [1, 256, 3, 3]               0               0\n",
            "   conv_block_nested-13       [1, 512, 3, 3]       3,542,016       3,542,016\n",
            "            Upsample-14       [1, 512, 6, 6]               0               0\n",
            "   conv_block_nested-15       [1, 256, 7, 7]       2,360,832       2,360,832\n",
            "            Upsample-16     [1, 256, 14, 14]               0               0\n",
            "   conv_block_nested-17     [1, 128, 14, 14]         738,048               0\n",
            "            Upsample-18     [1, 128, 28, 28]               0               0\n",
            "   conv_block_nested-19      [1, 64, 28, 28]         221,568               0\n",
            "           MaxPool2d-20       [1, 512, 1, 1]               0               0\n",
            "   conv_block_nested-21      [1, 1024, 1, 1]      14,161,920               0\n",
            "            Upsample-22      [1, 1024, 2, 2]               0               0\n",
            "   conv_block_nested-23       [1, 512, 3, 3]       9,440,256               0\n",
            "            Upsample-24       [1, 512, 6, 6]               0               0\n",
            "   conv_block_nested-25       [1, 256, 7, 7]       2,950,656               0\n",
            "            Upsample-26     [1, 256, 14, 14]               0               0\n",
            "   conv_block_nested-27     [1, 128, 14, 14]         885,504               0\n",
            "            Upsample-28     [1, 128, 28, 28]               0               0\n",
            "   conv_block_nested-29      [1, 64, 28, 28]         258,432               0\n",
            "              Conv2d-30       [1, 2, 28, 28]             130               0\n",
            "=============================================================================\n",
            "Total params: 36,628,546\n",
            "Trainable params: 7,972,032\n",
            "Non-trainable params: 28,656,514\n",
            "-----------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOjt1tohyr6o",
        "colab_type": "text"
      },
      "source": [
        "----------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPSZY0o4yvUC",
        "colab_type": "text"
      },
      "source": [
        "FC DenseNet finetuning freezing layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvc9AL8fyuyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DenseLayer(nn.Sequential):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(in_channels))\n",
        "        self.add_module('relu', nn.ReLU(True))\n",
        "        self.add_module('conv', nn.Conv2d(in_channels, growth_rate, kernel_size=3,\n",
        "                                          stride=1, padding=1, bias=True))\n",
        "        self.add_module('drop', nn.Dropout2d(0.2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x)\n",
        "\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, n_layers, upsample=False):\n",
        "        super().__init__()\n",
        "        self.upsample = upsample\n",
        "        self.layers = nn.ModuleList([DenseLayer(\n",
        "            in_channels + i*growth_rate, growth_rate)\n",
        "            for i in range(n_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.upsample:\n",
        "            new_features = []\n",
        "            #we pass all previous activations into each dense layer normally\n",
        "            #But we only store each dense layer's output in the new_features array\n",
        "            for layer in self.layers:\n",
        "                out = layer(x)\n",
        "                x = torch.cat([x, out], 1)\n",
        "                new_features.append(out)\n",
        "            return torch.cat(new_features,1)\n",
        "        else:\n",
        "            for layer in self.layers:\n",
        "                out = layer(x)\n",
        "                x = torch.cat([x, out], 1) # 1 = channel axis\n",
        "            return x\n",
        "\n",
        "\n",
        "class TransitionDown(nn.Sequential):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_features=in_channels))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(in_channels, in_channels,\n",
        "                                          kernel_size=1, stride=1,\n",
        "                                          padding=0, bias=True))\n",
        "        self.add_module('drop', nn.Dropout2d(0.2))\n",
        "        self.add_module('maxpool', nn.MaxPool2d(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x)\n",
        "\n",
        "\n",
        "class TransitionUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.convTrans = nn.ConvTranspose2d(\n",
        "            in_channels=in_channels, out_channels=out_channels,\n",
        "            kernel_size=3, stride=2, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        out = self.convTrans(x)\n",
        "        out = center_crop(out, skip.size(2), skip.size(3))\n",
        "        out = torch.cat([out, skip], 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Sequential):\n",
        "    def __init__(self, in_channels, growth_rate, n_layers):\n",
        "        super().__init__()\n",
        "        self.add_module('bottleneck', DenseBlock(\n",
        "            in_channels, growth_rate, n_layers, upsample=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return super().forward(x)\n",
        "\n",
        "\n",
        "def center_crop(layer, max_height, max_width):\n",
        "    _, _, h, w = layer.size()\n",
        "    xy1 = (w - max_width) // 2\n",
        "    xy2 = (h - max_height) // 2\n",
        "    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\n",
        "\n",
        "\n",
        "class FCDenseNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, down_blocks=(5,5,5,5,5),\n",
        "                 up_blocks=(5,5,5,5,5), bottleneck_layers=5,\n",
        "                 growth_rate=16, out_chans_first_conv=48, n_classes=2):\n",
        "        super().__init__()\n",
        "        self.down_blocks = down_blocks\n",
        "        self.up_blocks = up_blocks\n",
        "        cur_channels_count = 0\n",
        "        skip_connection_channel_counts = []\n",
        "\n",
        "        ## First Convolution ##\n",
        "\n",
        "        self.add_module('firstconv', nn.Conv2d(in_channels=in_channels,\n",
        "                  out_channels=out_chans_first_conv, kernel_size=3,\n",
        "                  stride=1, padding=1, bias=True))\n",
        "        cur_channels_count = out_chans_first_conv\n",
        "\n",
        "        #####################\n",
        "        # Downsampling path #\n",
        "        #####################\n",
        "\n",
        "        self.denseBlocksDown = nn.ModuleList([])\n",
        "        self.transDownBlocks = nn.ModuleList([])\n",
        "        for i in range(len(down_blocks)):\n",
        "            self.denseBlocksDown.append(\n",
        "                DenseBlock(cur_channels_count, growth_rate, down_blocks[i]))\n",
        "            cur_channels_count += (growth_rate*down_blocks[i])\n",
        "            skip_connection_channel_counts.insert(0,cur_channels_count)\n",
        "            self.transDownBlocks.append(TransitionDown(cur_channels_count))\n",
        "\n",
        "        #####################\n",
        "        #     Bottleneck    #\n",
        "        #####################\n",
        "\n",
        "        self.add_module('bottleneck',Bottleneck(cur_channels_count,\n",
        "                                     growth_rate, bottleneck_layers))\n",
        "        prev_block_channels = growth_rate*bottleneck_layers\n",
        "        cur_channels_count += prev_block_channels\n",
        "\n",
        "        #######################\n",
        "        #   Upsampling path   #\n",
        "        #######################\n",
        "\n",
        "        self.transUpBlocks = nn.ModuleList([])\n",
        "        self.denseBlocksUp = nn.ModuleList([])\n",
        "        for i in range(len(up_blocks)-1):\n",
        "            self.transUpBlocks.append(TransitionUp(prev_block_channels, prev_block_channels))\n",
        "            cur_channels_count = prev_block_channels + skip_connection_channel_counts[i]\n",
        "\n",
        "            self.denseBlocksUp.append(DenseBlock(\n",
        "                cur_channels_count, growth_rate, up_blocks[i],\n",
        "                    upsample=True))\n",
        "            prev_block_channels = growth_rate*up_blocks[i]\n",
        "            cur_channels_count += prev_block_channels\n",
        "\n",
        "        ## Final DenseBlock ##\n",
        "\n",
        "        self.transUpBlocks.append(TransitionUp(\n",
        "            prev_block_channels, prev_block_channels))\n",
        "        cur_channels_count = prev_block_channels + skip_connection_channel_counts[-1]\n",
        "\n",
        "        self.denseBlocksUp.append(DenseBlock(\n",
        "            cur_channels_count, growth_rate, up_blocks[-1],\n",
        "                upsample=False))\n",
        "        cur_channels_count += growth_rate*up_blocks[-1]\n",
        "\n",
        "        ## Softmax ##\n",
        "\n",
        "        self.finalConv = nn.Conv2d(in_channels=cur_channels_count,\n",
        "               out_channels=n_classes, kernel_size=1, stride=1,\n",
        "                   padding=0, bias=True)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.firstconv(x)\n",
        "\n",
        "        skip_connections = []\n",
        "        for i in range(len(self.down_blocks)):\n",
        "            out = self.denseBlocksDown[i](out)\n",
        "            skip_connections.append(out)\n",
        "            out = self.transDownBlocks[i](out)\n",
        "\n",
        "        out = self.bottleneck(out)\n",
        "        for i in range(len(self.up_blocks)):\n",
        "            skip = skip_connections.pop()\n",
        "            out = self.transUpBlocks[i](out, skip)\n",
        "            out = self.denseBlocksUp[i](out)\n",
        "\n",
        "        out = self.finalConv(out)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def FCDenseNet57(n_classes):\n",
        "    return FCDenseNet(\n",
        "        in_channels=1, down_blocks=(4, 4, 4, 4, 4),\n",
        "        up_blocks=(4, 4, 4, 4, 4), bottleneck_layers=4,\n",
        "        growth_rate=12, out_chans_first_conv=48, n_classes=n_classes)\n",
        "\n",
        "\n",
        "def FCDenseNet67(n_classes):\n",
        "    return FCDenseNet(\n",
        "        in_channels=1, down_blocks=(5, 5, 5, 5, 5),\n",
        "        up_blocks=(5, 5, 5, 5, 5), bottleneck_layers=5,\n",
        "        growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)\n",
        "\n",
        "\n",
        "def FCDenseNet103(n_classes):\n",
        "    return FCDenseNet(\n",
        "        in_channels=1, down_blocks=(4,5,7,10,12),\n",
        "        up_blocks=(12,10,7,5,4), bottleneck_layers=15,\n",
        "        growth_rate=16, out_chans_first_conv=48, n_classes=n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-etXyG5zGKB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5d71c937-d66c-4928-bdfb-de17de749871"
      },
      "source": [
        "model = FCDenseNet103(n_classes=2)\n",
        "for name, child in model.named_children():\n",
        "    print(name)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "firstconv\n",
            "denseBlocksDown\n",
            "transDownBlocks\n",
            "bottleneck\n",
            "transUpBlocks\n",
            "denseBlocksUp\n",
            "finalConv\n",
            "softmax\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr9do55NzMFc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b8ce6d48-5400-4a62-a591-c63d3aae2fd7"
      },
      "source": [
        "for name, child in model.named_children():\n",
        "   if name in ['softmax','firstconv','denseBlocksDown','transDownBlocks']:\n",
        "       print(name + ' is unfrozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = True\n",
        "   else:\n",
        "       print(name + ' is frozen')\n",
        "       for param in child.parameters():\n",
        "           param.requires_grad = False"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "firstconv is unfrozen\n",
            "denseBlocksDown is unfrozen\n",
            "transDownBlocks is unfrozen\n",
            "bottleneck is frozen\n",
            "transUpBlocks is frozen\n",
            "denseBlocksUp is frozen\n",
            "finalConv is frozen\n",
            "softmax is unfrozen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdRa6z2AzPhw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "1f71134b-4417-478a-ae1a-ee23fc5034cd"
      },
      "source": [
        "!pip install pytorch-model-summary\n",
        "from pytorch_model_summary import summary\n",
        "print(summary(model, torch.zeros((1, 1, 250, 250)), show_input=False))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-model-summary in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-model-summary) (1.5.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-model-summary) (0.16.0)\n",
            "--------------------------------------------------------------------------\n",
            "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
            "==========================================================================\n",
            "          Conv2d-1      [1, 48, 250, 250]             480             480\n",
            "      DenseBlock-2     [1, 112, 250, 250]          42,112          42,112\n",
            "     BatchNorm2d-3     [1, 112, 250, 250]             224             224\n",
            "            ReLU-4     [1, 112, 250, 250]               0               0\n",
            "          Conv2d-5     [1, 112, 250, 250]          12,656          12,656\n",
            "       Dropout2d-6     [1, 112, 250, 250]               0               0\n",
            "       MaxPool2d-7     [1, 112, 125, 125]               0               0\n",
            "      DenseBlock-8     [1, 192, 125, 125]         105,200         105,200\n",
            "     BatchNorm2d-9     [1, 192, 125, 125]             384             384\n",
            "           ReLU-10     [1, 192, 125, 125]               0               0\n",
            "         Conv2d-11     [1, 192, 125, 125]          37,056          37,056\n",
            "      Dropout2d-12     [1, 192, 125, 125]               0               0\n",
            "      MaxPool2d-13       [1, 192, 62, 62]               0               0\n",
            "     DenseBlock-14       [1, 304, 62, 62]         245,392         245,392\n",
            "    BatchNorm2d-15       [1, 304, 62, 62]             608             608\n",
            "           ReLU-16       [1, 304, 62, 62]               0               0\n",
            "         Conv2d-17       [1, 304, 62, 62]          92,720          92,720\n",
            "      Dropout2d-18       [1, 304, 62, 62]               0               0\n",
            "      MaxPool2d-19       [1, 304, 31, 31]               0               0\n",
            "     DenseBlock-20       [1, 464, 31, 31]         549,120         549,120\n",
            "    BatchNorm2d-21       [1, 464, 31, 31]             928             928\n",
            "           ReLU-22       [1, 464, 31, 31]               0               0\n",
            "         Conv2d-23       [1, 464, 31, 31]         215,760         215,760\n",
            "      Dropout2d-24       [1, 464, 31, 31]               0               0\n",
            "      MaxPool2d-25       [1, 464, 15, 15]               0               0\n",
            "     DenseBlock-26       [1, 656, 15, 15]         967,296         967,296\n",
            "    BatchNorm2d-27       [1, 656, 15, 15]           1,312           1,312\n",
            "           ReLU-28       [1, 656, 15, 15]               0               0\n",
            "         Conv2d-29       [1, 656, 15, 15]         430,992         430,992\n",
            "      Dropout2d-30       [1, 656, 15, 15]               0               0\n",
            "      MaxPool2d-31         [1, 656, 7, 7]               0               0\n",
            "     DenseBlock-32         [1, 240, 7, 7]       1,682,160               0\n",
            "   TransitionUp-33       [1, 896, 15, 15]         518,640               0\n",
            "     DenseBlock-34       [1, 192, 15, 15]       1,724,160               0\n",
            "   TransitionUp-35       [1, 656, 31, 31]         331,968               0\n",
            "     DenseBlock-36       [1, 160, 31, 31]       1,063,040               0\n",
            "   TransitionUp-37       [1, 464, 62, 62]         230,560               0\n",
            "     DenseBlock-38       [1, 112, 62, 62]         523,376               0\n",
            "   TransitionUp-39     [1, 304, 125, 125]         113,008               0\n",
            "     DenseBlock-40      [1, 80, 125, 125]         245,360               0\n",
            "   TransitionUp-41     [1, 192, 250, 250]          57,680               0\n",
            "     DenseBlock-42     [1, 256, 250, 250]         126,208               0\n",
            "         Conv2d-43       [1, 2, 250, 250]             514               0\n",
            "     LogSoftmax-44       [1, 2, 250, 250]               0               0\n",
            "==========================================================================\n",
            "Total params: 9,318,914\n",
            "Trainable params: 2,702,240\n",
            "Non-trainable params: 6,616,674\n",
            "--------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOtRHc9AzRBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}